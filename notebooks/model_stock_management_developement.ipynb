{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C0BkZXt65Ez",
        "outputId": "23a1bc40-2ac9-4a90-9040-db4f19ee38d6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath('../src'))\n",
        "\n",
        "from data_preprocessing_utils import data_for_training\n",
        "from utils import compute_error\n",
        "from models import get_mlp_model\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tabulate import tabulate\n",
        "\n",
        "import optuna\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3bTXF_XcinP",
        "outputId": "f76c02a0-521f-4017-c706-a3cf084c3307"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F0LEKbphcinQ"
      },
      "source": [
        "# We start by loading the pre-processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iok0znZ9cinS",
        "outputId": "13141a83-357f-41a0-df1b-e57456394b51"
      },
      "outputs": [],
      "source": [
        "sales_and_purchase_prices=pd.read_csv('../data/prepocessing/sales_and_purchase_prices.csv')\n",
        "sales_and_purchase_prices.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "StUkoi-JcinT"
      },
      "source": [
        "# we aggregate sales by month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "0idcWQQRBwit",
        "outputId": "a12bf860-ca74-403d-dd15-25f2081370a6"
      },
      "outputs": [],
      "source": [
        "# Removal of some columns that are not important for prediction: [\"InventoryId\",\"Brand\",\"Volume\",\"VendorNo\",\"Amount\",\"VendorName\"]\n",
        "sales_and_purchase_prices=sales_and_purchase_prices.drop([\"InventoryId\",\"Brand\",\"Volume\",\"VendorNo\",\"Amount\",\"VendorName\",\"SalesDollars\"], axis=1)\n",
        "sales_and_purchase_prices.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TijewIBdM1Ey",
        "outputId": "9c915e28-bc03-431b-e24a-6c1cd0656143"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert date columns to datetime format\n",
        "sales_and_purchase_prices['SalesDate'] = pd.to_datetime(sales_and_purchase_prices['SalesDate'])\n",
        "\n",
        "# Créez une nouvelle colonne pour l'année et le mois\n",
        "sales_and_purchase_prices['Year'] = sales_and_purchase_prices['SalesDate'].dt.year\n",
        "sales_and_purchase_prices['Month'] = sales_and_purchase_prices['SalesDate'].dt.month\n",
        "\n",
        "# Groupez les données par année et par mois et calculez les agrégats souhaités pour chaque groupe\n",
        "sales_month_aggregated=sales_and_purchase_prices.groupby(['Year', 'Month','Description','Store','Classification']).agg({\n",
        "    'SalesQuantity': 'sum',\n",
        "    'ExciseTax': 'first',\n",
        "    'Size': 'mean',\n",
        "    'PurchasePrice': 'first',\n",
        "    'SalesPrice':'first',\n",
        "}).reset_index()\n",
        "\n",
        "sales_month_aggregated.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "accsf3FX-psn"
      },
      "source": [
        "# Deep learning model optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scn8bL42tsdw"
      },
      "outputs": [],
      "source": [
        "# Assuming sales_month_aggregated is your DataFrame\n",
        "data = sales_month_aggregated.copy()\n",
        "\n",
        "#get model input data through the function data_for_training\n",
        "X_train,X_test,X_val,y_train,y_test,y_val=data_for_training(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9xS2bFBycinX"
      },
      "source": [
        "We're going to use mlp. To start with, we'll create an mlp with minimal parameters to get an idea of performance without model optimization. Then we'll optmize our model using Bayesian optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWk8p6_LcinX",
        "outputId": "692b591b-a8c3-4db4-c394-bcb350fd4cef"
      },
      "outputs": [],
      "source": [
        "\n",
        "best_mlp = MLPRegressor(hidden_layer_sizes=(50), activation='relu', alpha=0.008724528119026307, learning_rate='constant')\n",
        "\n",
        "# Train the model on the training data\n",
        "best_mlp.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set with the trained model\n",
        "y_val_pred = best_mlp.predict(X_val)\n",
        "\n",
        "# Evaluate the performance of the model on the validation set\n",
        "mse_val = mean_squared_error(y_val, y_val_pred)\n",
        "r2_val = best_mlp.score(X_val, y_val)\n",
        "\n",
        "# Make predictions on the test set with the trained model\n",
        "y_test_pred = best_mlp.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model on the test set\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "r2_test = best_mlp.score(X_test, y_test)\n",
        "\n",
        "print(\"Validation Set:\")\n",
        "print(\"MSE (Mean Squared Error) on the validation set:\", mse_val)\n",
        "print(\"R-squared (Coefficient of Determination) on the validation set:\", r2_val)\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(\"MSE (Mean Squared Error) on the test set:\", mse_test)\n",
        "print(\"R-squared (Coefficient of Determination) on the test set:\", r2_test)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "spupqHY9cinY"
      },
      "source": [
        "Using Bayesian optimization to find the best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr-pESuVcinY",
        "outputId": "599630c9-c293-468e-d074-15241ec7bb5e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Objective function for Optuna optimization\n",
        "def objective(trial):\n",
        "    # Define the hyperparameters to be optimized\n",
        "    hidden_layer_one = trial.suggest_categorical('hidden_layer_one', [32, 64, 128, 256,512])\n",
        "    hidden_layer_two = trial.suggest_categorical('hidden_layer_two', [32, 64, 128, 256,512])\n",
        "    dropout_one = trial.suggest_uniform('dropout_one', 0, 0.5)\n",
        "    dropout_two = trial.suggest_uniform('dropout_two', 0, 0.5)\n",
        "    # learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128,256,512])\n",
        "    epochs = trial.suggest_categorical('epochs', [200])\n",
        "\n",
        "\n",
        "\n",
        "    model=get_mlp_model(input_shape=(X_train.shape[1],),hidden_layer_one=hidden_layer_one,dropout_one=dropout_one,hidden_layer_two=hidden_layer_two,dropout_two=dropout_two)\n",
        "\n",
        "\n",
        "    # Define the ModelCheckpoint callback\n",
        "    checkpoint_filepath = 'model_checkpoint.h5'\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train the model on the training data with validation data and checkpoint callback\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0\n",
        "                        , validation_data=(X_val, y_val),\n",
        "                        callbacks=[model_checkpoint])\n",
        "\n",
        "    # Load the best weights from the saved checkpoint\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "\n",
        "    # Evaluate the performance of the model on the test set\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "    # rmse2, corr2, mae2, rae2, rrse2, mape2, r2_2 = compute_error(y_test.values, y_test_pred.reshape(y_test_pred.shape[0]))\n",
        "\n",
        "    return mse_test\n",
        "\n",
        "# Configure Optuna to use the GPU for exhaustive searches\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100, n_jobs=1)  # Use n_jobs=1 to avoid parallelism problems on GPU\n",
        "\n",
        "# Show best hyperparameters found\n",
        "print(\"Best hyperparameters:\")\n",
        "print(study.best_params)\n",
        "print(\"Best MSE:\", study.best_value)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MFAmNGI7cinY"
      },
      "source": [
        "Using Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPegPLSbcinZ",
        "outputId": "c62e9bfd-a0e4-45ec-c3b7-6f61f0a9bb4f"
      },
      "outputs": [],
      "source": [
        "\n",
        "r2=0\n",
        "\n",
        "for hidden_layer_one in [25,50,75,100,125,150,175,200,225,350,300,350,400]:\n",
        "  for hidden_layer_two in [325,50,75,100,125,150,175,200,225,350,300,350,400]:\n",
        "      for dropout_one in [0,0.1,0.2,0.3,0.4,0.5]:\n",
        "        for dropout_two in [0,0.1,0.2,0.3,0.4,0.5]:\n",
        "          # for batch_size in [32,64,128,256]:\n",
        "\n",
        "\n",
        "            model=get_mlp_model(input_shape=(X_train.shape[1],),hidden_layer_one=hidden_layer_one,dropout_one=dropout_one,hidden_layer_two=hidden_layer_two,dropout_two=dropout_two)\n",
        "\n",
        "            # Define the ModelCheckpoint callback\n",
        "            checkpoint_filepath = 'model_checkpoint.h5'\n",
        "            model_checkpoint = ModelCheckpoint(\n",
        "                filepath=checkpoint_filepath,\n",
        "                save_best_only=True,\n",
        "                monitor='val_loss',\n",
        "                mode='min',\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Train the model on the training data with validation data and checkpoint callback\n",
        "            history = model.fit(X_train, y_train, epochs=300, batch_size=256,\n",
        "                                verbose=0, validation_data=(X_val, y_val),\n",
        "                                callbacks=[model_checkpoint])\n",
        "\n",
        "            # Load the best weights from the saved checkpoint\n",
        "            model.load_weights(checkpoint_filepath)\n",
        "\n",
        "            # Evaluate the performance of the model on the test set\n",
        "            y_test_pred = model.predict(X_test)\n",
        "\n",
        "            # Calculate performance metrics\n",
        "            mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "            rmse2, corr2, mae2, rae2, rrse2, mape2, r2_2 = compute_error(y_test.values, y_test_pred.reshape(y_test_pred.shape[0]))\n",
        "\n",
        "            if r2_2>r2:\n",
        "              r2=r2_2\n",
        "              print(\"{},{},{},{}\".format(hidden_layer_one,hidden_layer_two,dropout_one,dropout_two) )\n",
        "              print(\"RMSE:\", rmse2)\n",
        "              print(\"Corrélation:\", corr2)\n",
        "              print(\"MAE:\", mae2)\n",
        "              print(\"RAE:\", rae2)\n",
        "              print(\"RRSE:\", rrse2)\n",
        "              print(\"MAPE:\", mape2)\n",
        "              print(\"R2:\", r2_2)\n",
        "              print(\"MSE:\", mse_test)\n",
        "              print(\"----------------------------------\")\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0fXRG0VmcinZ"
      },
      "source": [
        "Training model with best hyperparamters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1l_cOCQyyXH",
        "outputId": "cc03c112-6f37-424a-8b44-1782197d1ff0"
      },
      "outputs": [],
      "source": [
        "# best parameters\n",
        "\n",
        "hidden_layer_one=50\n",
        "hidden_layer_two=100\n",
        "dropout_one=0\n",
        "dropout_two=0\n",
        "\n",
        "\n",
        "model=get_mlp_model(input_shape=(X_train.shape[1],),hidden_layer_one=hidden_layer_one,dropout_one=dropout_one,hidden_layer_two=hidden_layer_two,dropout_two=dropout_two)\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "checkpoint_filepath = 'model_checkpoint.h5'\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "filepath=checkpoint_filepath,\n",
        "save_best_only=True,\n",
        "monitor='val_loss',\n",
        "mode='min',\n",
        "verbose=0\n",
        ")\n",
        "\n",
        "# Train the model on the training data with validation data and checkpoint callback\n",
        "history = model.fit(X_train, y_train, epochs=300, batch_size=512,\n",
        "            verbose=2, validation_data=(X_val, y_val),\n",
        "            callbacks=[model_checkpoint])\n",
        "\n",
        "# Load the best weights from the saved checkpoint\n",
        "model.load_weights(checkpoint_filepath)\n",
        "\n",
        "# Evaluate the performance of the model on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "rmse2, corr2, mae2, rae2, rrse2, mape2, r2_2 = compute_error(y_test.values, y_test_pred.reshape(y_test_pred.shape[0]))\n",
        "\n",
        "print(\"RMSE:\", rmse2)\n",
        "print(\"Corrélation:\", corr2)\n",
        "print(\"MAE:\", mae2)\n",
        "print(\"RAE:\", rae2)\n",
        "print(\"RRSE:\", rrse2)\n",
        "print(\"MAPE:\", mape2)\n",
        "print(\"R2:\", r2_2)\n",
        "print(\"----------------------------------\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AN-8AO2Ecina"
      },
      "source": [
        "Save the weights of the best model for later loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUAXH6qwi2By"
      },
      "outputs": [],
      "source": [
        "model.save_weights('optimized_model_weights.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
